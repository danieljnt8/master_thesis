{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "788da4f8-0135-4ceb-825a-4521a8521d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from trajectory.utils.common import round_to_multiple\n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import trange, tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_from_disk\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from trajectory.planning.sample import sample\n",
    "\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from trajectory.models.gpt import GPT, GPTTrainer\n",
    "from trajectory.utils.common import pad_along_axis\n",
    "from trajectory.utils.discretization import KBinsDiscretizer\n",
    "from trajectory.utils.env import create_env\n",
    "from trajectory.utils.CityStableEnv import EnvCityGym\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7fd104-168b-45d5-be70-3ac49dfd9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "checkpoints_path = 'checkpoints/city_learn/uniform/baseline'\n",
    "schema =  \"citylearn_challenge_2022_phase_2\"\n",
    "config = \"configs/eval_base.yaml\"\n",
    "config = OmegaConf.load(config)\n",
    "run_config= \"configs/medium/city_learn_traj.yaml\"\n",
    "run_config = OmegaConf.load(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f90804-7896-4335-b445-96d3fb324335",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_context = config.beam_context\n",
    "beam_width = config.beam_width\n",
    "beam_steps = config.beam_steps\n",
    "plan_every = config.plan_every\n",
    "sample_expand = config.sample_expand\n",
    "k_act = config.k_act\n",
    "k_obs = config.k_obs\n",
    "k_reward = config.k_reward\n",
    "temperature = config.temperature\n",
    "discount = config.discount\n",
    "max_steps = 719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4219cd2f-bc22-40d0-bbf7-ee16f2910819",
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = torch.load(os.path.join(checkpoints_path, \"discretizer.pt\"), map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf4a2fd-8c7e-4e28-aeac-402d06d9fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(**run_config.model)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoints_path, \"model_last.pt\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "79f428a1-0075-4449-bc7b-75550cd155f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema=schema)\n",
    "env = EnvCityGym(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "707923c1-3647-4a06-8a19-0310db93af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "151b9400-3712-47b9-8dab-9927e06e24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "63ea6922-5fdb-444c-bb82-516e10c0921c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.83333333e-01, 1.00000000e+00, 8.53622034e-02, 1.82999992e-01,\n",
       "       8.10000000e-01, 2.19999999e-01, 2.98903322e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.98903322e-01, 1.54143333e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.54143333e-01, 1.95058192e-08, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.95058192e-08, 1.26090002e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.26090002e-01, 1.09140003e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.09140003e-01])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "189966be-b9fc-4928-93fb-be4561081280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(3300, 128)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x TransformerBlock(\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): EinLinear(n_models=33, in_features=128, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ea9171f3-8fcd-4b8e-a947-3cbd9ddfb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " transition_dim, obs_dim, act_dim = model.transition_dim, model.observation_dim, model.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9273f4a0-f288-40b5-a1ef-b380466d0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros(1, model.transition_dim * (max_steps + 1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "98218df6-b122-4eac-95a2-ce22b1ef3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tokens = discretizer.encode(obs, subslice=(0, obs_dim)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "851e37d6-6598-48f7-b5c9-ae63d9756634",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[:, :model.observation_dim] = torch.as_tensor(obs_tokens, device=device)  # initial tokens for planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "727add9c-76c2-4292-a1c1-cc822a9a6398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[54, 99, 47,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c556e7f1-3922-48f3-9e64-997bfbc6391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "done, total_reward, render_frames = False, 0.0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e523b73b-1d0f-42a6-808c-765196532489",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_offset = model.transition_dim * (0 + 1) - model.action_dim - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bdf3f45c-a3f2-4b2a-9d54-88ab64f58204",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_bm = context[:, :context_offset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64c671-fd3e-4385-bcfa-596a0187e66d",
   "metadata": {},
   "source": [
    "## Beam Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "277dd706-c428-4487-b8c5-bb505fe7ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 3\n",
    "steps = beam_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "fa305273-3099-467a-843f-d991ae97a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.zeros(beam_width, steps + 1, device=context.device)\n",
    "discounts = discount ** torch.arange(steps + 1, device=context.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "524e42a3-c0f3-4e89-b60e-41bbfac07e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "context_size = context_size * model.transition_dim\n",
    "n_crop = round_to_multiple(max(0, context_bm.shape[1] - context_size), model.transition_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cec09953-30d7-495d-a9af-04141e350482",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_bm = context_bm[:, n_crop:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "28eb1e72-465b-4070-96c3-72b0189fd5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_bm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dc80560d-8e3e-42a6-a25f-8a1aa74825c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = context_bm.repeat(beam_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "90285344-e1ac-448d-aa0e-db196f55da7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -10.5940, -607.4584],\n",
      "        [  -7.0893, -623.1907],\n",
      "        [ -10.9646, -614.9783],\n",
      "        [ -12.4505, -607.2290],\n",
      "        [  -8.4440, -615.8477],\n",
      "        [ -10.4521, -605.1689]], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "---------------\n",
      "tensor([[ -10.5940, -607.4584,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  -7.0893, -623.1907,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.9646, -614.9783,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -12.4505, -607.2290,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [  -8.4440, -615.8477,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.4521, -605.1689,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------ Final Plan------------------\n",
      "torch.Size([3, 33])\n",
      "------ Final Rewards ---------\n",
      "tensor([[ -10.4521, -605.1689,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.5940, -607.4584,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [ -12.4505, -607.2290,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "-------------------------------\n",
      "tensor([[  -8.5616, -619.9463],\n",
      "        [  -5.6214, -592.3706],\n",
      "        [  -5.9174, -613.1945],\n",
      "        [ -10.8378, -602.9898],\n",
      "        [ -10.3543, -595.2358],\n",
      "        [ -13.5686, -605.2998]], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "---------------\n",
      "tensor([[ -10.4521,   -8.5616, -619.9463,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.5940,   -5.6214, -592.3706,    0.0000,    0.0000,    0.0000],\n",
      "        [ -12.4505,   -5.9174, -613.1945,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.4521,  -10.8378, -602.9899,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.5940,  -10.3543, -595.2358,    0.0000,    0.0000,    0.0000],\n",
      "        [ -12.4505,  -13.5686, -605.2998,    0.0000,    0.0000,    0.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------ Final Plan------------------\n",
      "torch.Size([3, 66])\n",
      "------ Final Rewards ---------\n",
      "tensor([[ -10.5940,   -5.6214, -592.3706,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.5940,  -10.3543, -595.2358,    0.0000,    0.0000,    0.0000],\n",
      "        [ -10.4521,  -10.8378, -602.9899,    0.0000,    0.0000,    0.0000]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "-------------------------------\n",
      "tensor([[  -3.0707, -580.2611],\n",
      "        [ -12.4475, -600.3971],\n",
      "        [  -9.2985, -603.1364],\n",
      "        [ -13.9717, -565.3702],\n",
      "        [ -15.7604, -584.0512],\n",
      "        [ -11.2546, -598.8685]], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "---------------\n",
      "tensor([[ -10.5940,   -5.6214,   -3.0707, -580.2611,    0.0000,    0.0000],\n",
      "        [ -10.5940,  -10.3543,  -12.4475, -600.3971,    0.0000,    0.0000],\n",
      "        [ -10.4521,  -10.8378,   -9.2985, -603.1364,    0.0000,    0.0000],\n",
      "        [ -10.5940,   -5.6214,  -13.9717, -565.3702,    0.0000,    0.0000],\n",
      "        [ -10.5940,  -10.3543,  -15.7604, -584.0512,    0.0000,    0.0000],\n",
      "        [ -10.4521,  -10.8378,  -11.2546, -598.8685,    0.0000,    0.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------ Final Plan------------------\n",
      "torch.Size([3, 99])\n",
      "------ Final Rewards ---------\n",
      "tensor([[ -10.5940,   -5.6214,  -13.9717, -565.3702,    0.0000,    0.0000],\n",
      "        [ -10.5940,   -5.6214,   -3.0707, -580.2611,    0.0000,    0.0000],\n",
      "        [ -10.5940,  -10.3543,  -15.7604, -584.0512,    0.0000,    0.0000]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "-------------------------------\n",
      "tensor([[  -7.1365, -580.8173],\n",
      "        [  -9.7780, -582.4634],\n",
      "        [  -6.9767, -578.6106],\n",
      "        [  -8.6123, -572.3537],\n",
      "        [  -9.9441, -573.7079],\n",
      "        [  -6.0166, -586.9647]], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "---------------\n",
      "tensor([[ -10.5940,   -5.6214,  -13.9717,   -7.1365, -580.8173,    0.0000],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780, -582.4634,    0.0000],\n",
      "        [ -10.5940,  -10.3543,  -15.7604,   -6.9767, -578.6105,    0.0000],\n",
      "        [ -10.5940,   -5.6214,  -13.9717,   -8.6123, -572.3537,    0.0000],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.9441, -573.7079,    0.0000],\n",
      "        [ -10.5940,  -10.3543,  -15.7604,   -6.0166, -586.9647,    0.0000]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------ Final Plan------------------\n",
      "torch.Size([3, 132])\n",
      "------ Final Rewards ---------\n",
      "tensor([[ -10.5940,   -5.6214,   -3.0707,   -9.9441, -573.7079,    0.0000],\n",
      "        [ -10.5940,   -5.6214,  -13.9717,   -8.6123, -572.3537,    0.0000],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780, -582.4634,    0.0000]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "-------------------------------\n",
      "tensor([[ -14.0912, -573.2366],\n",
      "        [  -5.6267, -590.1446],\n",
      "        [  -9.4717, -589.2567],\n",
      "        [  -6.0763, -601.4705],\n",
      "        [  -6.7031, -603.2826],\n",
      "        [ -14.5432, -574.4642]], dtype=torch.float64, grad_fn=<SumBackward1>)\n",
      "---------------\n",
      "tensor([[ -10.5940,   -5.6214,   -3.0707,   -9.9441,  -14.0912, -573.2366],\n",
      "        [ -10.5940,   -5.6214,  -13.9717,   -8.6123,   -5.6267, -590.1446],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780,   -9.4717, -589.2567],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.9441,   -6.0763, -601.4705],\n",
      "        [ -10.5940,   -5.6214,  -13.9717,   -8.6123,   -6.7031, -603.2825],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780,  -14.5432, -574.4642]],\n",
      "       grad_fn=<CopySlices>)\n",
      "------------ Final Plan------------------\n",
      "torch.Size([3, 165])\n",
      "------ Final Rewards ---------\n",
      "tensor([[ -10.5940,   -5.6214,   -3.0707,   -9.9441,  -14.0912, -573.2366],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780,  -14.5432, -574.4642],\n",
      "        [ -10.5940,   -5.6214,   -3.0707,   -9.7780,   -9.4717, -589.2567]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in trange(steps, leave=False):\n",
    "        # [beam_width * sample_expand, ...]\n",
    "    plan = plan.repeat(sample_expand, 1)\n",
    "    rewards = rewards.repeat(sample_expand, 1)\n",
    "\n",
    "    #if model_state is not None:\n",
    "            # [beam_width * sample_expand, cache_len, emb_dim]\n",
    "    #   model_state = [s.repeat(sample_expand, 1, 1) for s in model_state]\n",
    "\n",
    "        # sample action tokens\n",
    "    plan, model_state, _ = sample(\n",
    "            model, plan, model_state=None, steps=model.action_dim, top_k=k_act, temperature=temperature\n",
    "        )\n",
    "        # sample reward and value estimates\n",
    "    plan, model_state, logits = sample(\n",
    "            model, plan, model_state=None, steps=2, top_k=k_reward, temperature=temperature\n",
    "        )\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    reward_and_value = discretizer.expectation(probs, subslice=[model.transition_dim - 2, model.transition_dim])\n",
    "    print(reward_and_value)\n",
    "    rewards[:, t:t + 2] = reward_and_value\n",
    "    print(\"---------------\")\n",
    "    print(rewards)\n",
    "    values = (rewards * discounts).sum(-1)\n",
    "    values, idxs = torch.topk(values, k=beam_width)\n",
    "\n",
    "    plan, rewards = plan[idxs], rewards[idxs]\n",
    "    \n",
    "    print(\"------------ Final Plan------------------\")\n",
    "    print(plan.size())\n",
    "    \n",
    "    print(\"------ Final Rewards ---------\")\n",
    "    print(rewards)\n",
    "    print(\"-------------------------------\")\n",
    "    model_state = [s[idxs] for s in model_state]\n",
    "\n",
    "    if t < steps - 1:\n",
    "            # sample obs unless last step\n",
    "        plan, model_state, _ = sample(\n",
    "                model, plan, model_state=model_state, steps=model.observation_dim, top_k=k_obs, temperature=temperature\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6cd0f511-c6da-4404-9006-2e589c7d00d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_bm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bb3dfb36-0d37-449e-a971-bacff0412a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -12.9115,  -16.2543,   -4.0252,   -2.7249,   -7.1054, -554.9442],\n",
       "        [ -12.9115,  -16.2543,   -4.0252,   -2.8037,   -5.7905, -560.5143],\n",
       "        [ -12.9115,  -16.2543,   -4.0252,   -2.7249,  -10.4618, -555.9832]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91e1fc9f-a924-4bfb-b593-c34688562258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-539.0653, -539.1727, -550.2013], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edf4a6-8fa5-413d-ad24-1653c24d00dc",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "770fcae4-5d9a-43a3-bf08-13afbb8cf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_expand = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b3b59a97-2836-46bb-a3ed-cf297e0e7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = plan.repeat(1, 1)\n",
    "rewards = rewards.repeat(sample_expand, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eb100163-1fe4-4c7d-940b-572d6c9dddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "68ba3f77-b545-4226-aa1d-51344b38b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just one action for further one \n",
    "test,ms = model(context[:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fdc8a0f7-279c-48ab-be38-5101fbf71a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbe16d-2b5c-4076-bd2a-5c11a2d05ef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2d2dcd15-babc-437d-8282-be51a7f95605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = context.shape[0]\n",
    "batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6dedf4b2-c768-4c8f-bba3-a1503cba8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logits = torch.zeros(batch_size, steps, model.vocab_size, device=context.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "90a6191f-7882-4d0f-b5c8-a585ca0db009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0290cd76-b6a6-4861-a45d-8c8d7b618c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits,model_state = model(context, state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bd431dcc-6fbf-4e5d-865e-948337309e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 100])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "680fb299-6b5d-4840-924e-444c1481a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_inner(logits, top_k, temperature, greedy=False):\n",
    "    logits = logits / temperature\n",
    "\n",
    "    if top_k is not None:\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    if greedy:\n",
    "        idx = torch.topk(probs, k=1, dim=-1)[-1]\n",
    "    else:\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ca65d927-7b5a-48ae-ab45-7e9a78d24b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c120ff11-5e78-4d6f-8492-2659b418d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_tokens = _sample_inner(logits[:, -1, :], None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "af6cf47d-c4da-4cc2-b133-62dff3bd9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " context = torch.hstack([context, sampled_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "439f4657-0c2a-475d-a3a7-37776666fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f0c52d87-b37c-4f1b-83b0-97d86da5a9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.6365, -15.5346, -15.5312,  ..., -15.4928, -15.5322,  -2.4405],\n",
       "         [ -8.1907,  -6.3107,  -4.2083,  ...,  -6.0341,  -8.8072,  -5.8740],\n",
       "         [ -6.3233,  -3.6427, -13.5979,  ..., -13.4999, -13.5672,  -8.4786],\n",
       "         ...,\n",
       "         [  1.1276,  -0.2794,  -0.5848,  ...,   0.8913,   1.1704,   1.9366],\n",
       "         [ -7.6923,  -6.4762,  -4.2407,  ..., -11.7560, -11.7819,  -8.5468],\n",
       "         [  1.8463,  -0.9704,  -1.5757,  ...,  -1.0963,  -1.5979,   1.9532]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e2dbfcda-9d4a-4587-8f72-645d484f3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_logits[:, 0] = logits[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6071624e-2d31-4a92-9fc7-3b19d4e1c618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b764e-cf89-41f7-835f-11b5db8897b0",
   "metadata": {},
   "source": [
    "### RESUMING STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3fb1f64f-341e-43dd-87f9-490fcf97b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan, model_state, _ = sample(\n",
    "            model, plan, model_state=None, steps=model.action_dim, top_k=k_act, temperature=temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4ae39628-6fc7-4f2e-a38e-c6f8c85d4b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb389b-fbf0-40fd-9730-4dfbb1299c9e",
   "metadata": {},
   "source": [
    "For Rewards and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1f237f8a-2ebc-4fd1-b2a7-202485a64118",
   "metadata": {},
   "outputs": [],
   "source": [
    " plan, model_state, logits = sample(\n",
    "            model, plan, model_state=model_state, steps=2, top_k=k_reward, temperature=temperature\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9966d60d-7709-4956-ac04-24878508f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "270daee6-3dae-4398-8172-0508795520a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 100])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "418e1aff-eaa8-4e8b-831f-e3c1f17e0731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -9.5088, -614.6954],\n",
       "        [ -12.1123, -602.8091],\n",
       "        [ -14.4683, -612.9071]], dtype=torch.float64, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretizer.expectation(probs, subslice=[model.transition_dim -2, model.transition_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f92617b0-f5cf-423b-b651-617aff278909",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_and_value = discretizer.expectation(probs, subslice=[model.transition_dim - 2, model.transition_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ffca230f-e4c2-416a-9048-fc80c2b134af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[:, 0:0 + 2] = reward_and_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "09635921-916a-4908-abab-01bf1aeb09b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -9.5088, -614.6954,    0.0000,    0.0000,    0.0000,    0.0000],\n",
       "        [ -12.1123, -602.8091,    0.0000,    0.0000,    0.0000,    0.0000],\n",
       "        [ -14.4683, -612.9071,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "032beda1-6d97-4890-9b27-076b00143558",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = (rewards * discounts).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "921335c5-192d-412c-99f1-80cd6ee8f76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-618.0573, -608.8934, -621.2463], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a9cad543-b295-41a3-be49-6dd23e0fcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " values, idxs = torch.topk(values, k=beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "fb62bda1-b0ef-49d3-8ff7-865854bebf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = [s[idxs] for s in model_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "890ea9c0-ec9b-4ed6-b9d6-99594db756d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "02325097-043b-4d02-a84f-46b1981c8062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "485130b9-75e6-4a51-bb72-f5a489548aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = torch.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5633874e-67ab-4567-ae16-38e3689a6bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8071a7b8-73cf-4372-9a0a-fbb0da8c2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tokens=plan[best_idx,context.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "aae63fe4-0ba6-4447-bd22-ea5ec1b7c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_tokens = prediction_tokens[:act_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6db58a7a-3a88-42b3-abcd-eaccf45d1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = discretizer.decode(action_tokens.cpu().numpy(), subslice=(obs_dim, obs_dim + act_dim)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "18bfa5d3-8e4d-4c17-a3e6-9c513d5dd581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99,  0.99, -0.93,  0.09,  0.37])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f54b51ce-d976-4435-a009-8f9d8bf2133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ea6a2265-930a-4171-a26d-f0aad084d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "92bcb271-db95-49c5-bab1-c18a81107e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tokens = discretizer.encode(obs, subslice=(0, obs_dim)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "db726a8b-72ec-417a-9b5b-dddac24b9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_placeholder = 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6ddd2f51-546c-4db7-a77a-7c33109698c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tokens = discretizer.encode(\n",
    "            np.array([reward, value_placeholder]),\n",
    "            subslice=(transition_dim - 2, transition_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820de61-cf2f-4450-933e-bcfdcb364f45",
   "metadata": {},
   "source": [
    "### NEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b33156c2-37b0-4f35-8dc7-efb26b0eb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "69ebb2ed-fbc1-43e6-8f22-35627b4603b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_offset = model.transition_dim * step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c95c6fdf-d93a-4f33-a11c-65c301e803e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[:, context_offset + obs_dim:context_offset + obs_dim + act_dim] = torch.as_tensor(action_tokens, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "339a7f31-b266-4d37-8580-8ac0d452f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[:, context_offset + transition_dim - 2:context_offset + transition_dim] = torch.as_tensor(reward_tokens, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "9c7c8f80-1d5a-4411-ba3f-7eaa62ee2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[:, context_offset + model.transition_dim:context_offset + model.transition_dim + model.observation_dim] = torch.as_tensor(obs_tokens, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "71f97090-4e85-467e-ad3b-b2ea076f7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "test,_ = model(context[:,:59])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "2a37a78e-fecb-498b-ba65-5801b3efed5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 59, 100])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ecb24-4a5e-43b6-81cb-c73b3f8810ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
