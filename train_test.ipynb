{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959675da-af32-4c24-98bf-787f04018f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:31: DeprecationWarning: private variables, such as '_EvaluationCondition__DEFAULT', will be normal attributes in 3.10\n",
      "  __DEFAULT = ''\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:32: DeprecationWarning: private variables, such as '_EvaluationCondition__STORAGE_SUFFIX', will be normal attributes in 3.10\n",
      "  __STORAGE_SUFFIX = '_without_storage'\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:33: DeprecationWarning: private variables, such as '_EvaluationCondition__PARTIAL_LOAD_SUFFIX', will be normal attributes in 3.10\n",
      "  __PARTIAL_LOAD_SUFFIX = '_and_partial_load'\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:34: DeprecationWarning: private variables, such as '_EvaluationCondition__PV_SUFFIX', will be normal attributes in 3.10\n",
      "  __PV_SUFFIX = '_and_pv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import trange, tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_from_disk\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "from stable_baselines3.a2c import A2C\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from trajectory.models.gpt import GPT, GPTTrainer\n",
    "\n",
    "from trajectory.utils.common import pad_along_axis\n",
    "from trajectory.utils.discretization import KBinsDiscretizer\n",
    "from trajectory.utils.env import create_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75010c2d-fce9-44bd-98d9-f6a9d29d2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_data_path = \"data/DT_data/test2_9.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a9b8f6-ba3b-453c-915d-defe02a2e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(offline_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b285135-9d9e-49e8-bab5-2223fd4ddb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7852e39-f52b-4213-a8a9-693544a31ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"observations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b8390d-c4e3-472b-b5be-1286decc4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_obs = dataset[\"observations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d2395a9-c1ef-4024-9209-492de7d35f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trajectory(states, actions, rewards, discount=0.99):\n",
    "    traj_length = states.shape[0]\n",
    "    # I can vectorize this for all dataset as once,\n",
    "    # but better to be safe and do it once and slow and right (and cache it)\n",
    "    print(\"Discount \"+str(discount))\n",
    "    discounts = (discount ** np.arange(traj_length))\n",
    "\n",
    "    values = np.zeros_like(rewards)\n",
    "    for t in range(traj_length):\n",
    "        # discounted return-to-go from state s_t:\n",
    "        # r_{t+1} + y * r_{t+2} + y^2 * r_{t+3} + ...\n",
    "        # .T as rewards of shape [len, 1], see https://github.com/Howuhh/faster-trajectory-transformer/issues/9\n",
    "        values[t] = (rewards[t + 1:].T * discounts[:-t - 1]).sum()\n",
    "\n",
    "    joined_transition = np.concatenate([states, actions, rewards, values], axis=-1)\n",
    "\n",
    "    return joined_transition\n",
    "\n",
    "\n",
    "def segment(states, actions, rewards, terminals):\n",
    "    assert len(states) == len(terminals)\n",
    "    trajectories = {}\n",
    "\n",
    "    episode_num = 0\n",
    "    for t in trange(len(terminals), desc=\"Segmenting\"):\n",
    "        if episode_num not in trajectories:\n",
    "            trajectories[episode_num] = {\n",
    "                \"states\": [],\n",
    "                \"actions\": [],\n",
    "                \"rewards\": []\n",
    "            }\n",
    "        \n",
    "        trajectories[episode_num][\"states\"].append(states[t])\n",
    "        trajectories[episode_num][\"actions\"].append(actions[t])\n",
    "        trajectories[episode_num][\"rewards\"].append(rewards[t])\n",
    "\n",
    "        if terminals[t]:\n",
    "            # next episode\n",
    "            episode_num = episode_num + 1\n",
    "\n",
    "    trajectories_lens = [len(v[\"states\"]) for k, v in trajectories.items()]\n",
    "\n",
    "    for t in trajectories:\n",
    "        trajectories[t][\"states\"] = np.stack(trajectories[t][\"states\"], axis=0)\n",
    "        trajectories[t][\"actions\"] = np.stack(trajectories[t][\"actions\"], axis=0)\n",
    "        trajectories[t][\"rewards\"] = np.stack(trajectories[t][\"rewards\"], axis=0)\n",
    "\n",
    "    return trajectories, trajectories_lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54443dc3-7999-43a4-8ab9-46485f2b85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_citylearn_ds(dataset):\n",
    "    trajectories = {}\n",
    "    for i in range(len(dataset[\"observations\"])):\n",
    "        trajectories[i] = {\n",
    "                    \"states\": dataset[\"observations\"][i],\n",
    "                    \"actions\": dataset[\"actions\"][i],\n",
    "                    \"rewards\": [[element] for element in dataset[\"rewards\"][i]]\n",
    "                }\n",
    "    trajectories_lens = [len(v[\"states\"]) for k, v in trajectories.items()]\n",
    "    for t in trajectories:\n",
    "        trajectories[t][\"states\"] = np.stack(trajectories[t][\"states\"], axis=0)\n",
    "        trajectories[t][\"actions\"] = np.stack(trajectories[t][\"actions\"], axis=0)\n",
    "        trajectories[t][\"rewards\"] = np.stack(trajectories[t][\"rewards\"], axis=0)\n",
    "\n",
    "    return trajectories,trajectories_lens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510e9503-bb27-4aaf-9018-7d12e48b6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories, traj_lengths = segment_citylearn_ds(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce249a6a-7025-483f-a8ff-00d01e331d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedDataset(Dataset):\n",
    "    def __init__(self, dataset,env_name=\"city_learn\", num_bins=100, seq_len=10, discount=0.99, strategy=\"uniform\", cache_path=\"data\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.discount = discount\n",
    "        self.num_bins = num_bins\n",
    "        self.dataset = dataset\n",
    "        self.env_name = env_name\n",
    "        \n",
    "        trajectories, traj_lengths = segment_citylearn_ds(self.dataset)\n",
    "        self.trajectories = trajectories\n",
    "        self.traj_lengths = traj_lengths\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_name = f\"{env_name}_{num_bins}_{seq_len}_{strategy}_{discount}\"\n",
    "\n",
    "        if cache_path is None or not os.path.exists(os.path.join(cache_path, self.cache_name)):\n",
    "            self.joined_transitions = []\n",
    "            for t in tqdm(trajectories, desc=\"Joining transitions\"):\n",
    "                self.joined_transitions.append(\n",
    "                    join_trajectory(trajectories[t][\"states\"], trajectories[t][\"actions\"], trajectories[t][\"rewards\"],discount = self.discount)\n",
    "                )\n",
    "\n",
    "            os.makedirs(os.path.join(cache_path), exist_ok=True)\n",
    "            # save cached version\n",
    "            with open(os.path.join(cache_path, self.cache_name), \"wb\") as f:\n",
    "                pickle.dump(self.joined_transitions, f)\n",
    "        else:\n",
    "            with open(os.path.join(cache_path, self.cache_name), \"rb\") as f:\n",
    "                self.joined_transitions = pickle.load(f)\n",
    "\n",
    "        self.discretizer = KBinsDiscretizer(\n",
    "            np.concatenate(self.joined_transitions, axis=0),\n",
    "            num_bins=num_bins,\n",
    "            strategy=strategy\n",
    "        )\n",
    "\n",
    "        # get valid indices for seq_len sampling\n",
    "        indices = []\n",
    "        for path_ind, length in enumerate(traj_lengths):\n",
    "            end = length - 1\n",
    "            for i in range(end):\n",
    "                indices.append((path_ind, i, i + self.seq_len))\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def get_env_name(self):\n",
    "        return self.env.name\n",
    "\n",
    "    def get_discretizer(self):\n",
    "        return self.discretizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx, start_idx, end_idx = self.indices[idx]\n",
    "        joined = self.joined_transitions[traj_idx][start_idx:end_idx]\n",
    "\n",
    "        loss_pad_mask = np.ones((self.seq_len, joined.shape[-1]))\n",
    "        if joined.shape[0] < self.seq_len:\n",
    "            # pad to seq_len if at the end of trajectory, mask for padding\n",
    "            loss_pad_mask[joined.shape[0]:] = 0\n",
    "            joined = pad_along_axis(joined, pad_to=self.seq_len, axis=0)\n",
    "\n",
    "        joined_discrete = self.discretizer.encode(joined).reshape(-1).astype(np.longlong)\n",
    "        loss_pad_mask = loss_pad_mask.reshape(-1)\n",
    "\n",
    "        return joined_discrete[:-1], joined_discrete[1:], loss_pad_mask[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2563-ceeb-45db-8eaa-0d2e5c059dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"configs/medium/city_learn.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973cc79-5ccc-4c4c-9782-7de73801f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "        **config.wandb,\n",
    "        config=dict(OmegaConf.to_container(config, resolve=True))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b90777-0a95-4956-b28d-9e2f94a841b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965889c-d0c1-4c21-b3c3-2b565af6faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DiscretizedDataset(dataset,discount = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb99f78-8c09-433d-8b99-3cdca7375ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69fc46-944f-4b23-a6cd-f53240b298b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa1c64-7c14-4223-8214-0bba243a5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tqdm(dataloader, desc=\"Epoch\", leave=False)):\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0202890-3ee0-43a8-91d7-ea70e48ff346",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2bacf-7f61-4b23-b4b0-c18cb7f1b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader.dataset.get_discretizer()\n",
    "#dataloader.dataset.joined_transitions[0].shape\n",
    "#last_elements = dataloader.dataset.joined_transitions[0][:,-1]\n",
    "#rewards = dataloader.dataset.joined_transitions[0][:,-2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b1182-1362-4043-83af-dff982be8311",
   "metadata": {},
   "source": [
    "## Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ecbb7-e58b-4729-95ec-bc05fd10d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"configs/medium/city_learn.yaml\"\n",
    "config = OmegaConf.load(\"configs/medium/city_learn.yaml\")\n",
    "trainer_conf = config.trainer\n",
    "data_conf = config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c8623-c354-4b04-83e0-a0d04387a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(**config.model)\n",
    "model.to(device)\n",
    "#trainer_conf.num_epochs_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820f32-9e11-44d2-af1e-a365833b84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = int(1e3 / len(datasets) * trainer_conf.num_epochs_ref)\n",
    "\n",
    "warmup_tokens = len(datasets) * data_conf.seq_len * config.model.transition_dim\n",
    "final_tokens = warmup_tokens * num_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9980348-2e40-434d-b028-5d8e38e7b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GPTTrainer(\n",
    "        final_tokens=final_tokens,\n",
    "        warmup_tokens=warmup_tokens,\n",
    "        action_weight=trainer_conf.action_weight,\n",
    "        value_weight=trainer_conf.value_weight,\n",
    "        reward_weight=trainer_conf.reward_weight,\n",
    "        learning_rate=trainer_conf.lr,\n",
    "        betas=trainer_conf.betas,\n",
    "        weight_decay=trainer_conf.weight_decay,\n",
    "        clip_grad=trainer_conf.clip_grad,\n",
    "        eval_seed=trainer_conf.eval_seed,\n",
    "        eval_every=trainer_conf.eval_every,\n",
    "        eval_episodes=trainer_conf.eval_episodes,\n",
    "        eval_temperature=trainer_conf.eval_temperature,\n",
    "        eval_discount=trainer_conf.eval_discount,\n",
    "        eval_plan_every=trainer_conf.eval_plan_every,\n",
    "        eval_beam_width=trainer_conf.eval_beam_width,\n",
    "        eval_beam_steps=trainer_conf.eval_beam_steps,\n",
    "        eval_beam_context=trainer_conf.eval_beam_context,\n",
    "        eval_sample_expand=trainer_conf.eval_sample_expand,\n",
    "        eval_k_obs=trainer_conf.eval_k_obs,  # as in original implementation\n",
    "        eval_k_reward=trainer_conf.eval_k_reward,\n",
    "        eval_k_act=trainer_conf.eval_k_act,\n",
    "        checkpoints_path=trainer_conf.checkpoints_path,\n",
    "        save_every=1,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511aa60-078e-4e85-93f9-b040566e8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "        model=model,\n",
    "        dataloader=dataloader,\n",
    "        num_epochs=num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784f898-8537-4056-b7f9-d7d763c65633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
