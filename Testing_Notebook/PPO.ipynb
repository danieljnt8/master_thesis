{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9387c887-5a92-4fbc-af1c-e4a3232bedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:31: DeprecationWarning: private variables, such as '_EvaluationCondition__DEFAULT', will be normal attributes in 3.10\n",
      "  __DEFAULT = ''\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:32: DeprecationWarning: private variables, such as '_EvaluationCondition__STORAGE_SUFFIX', will be normal attributes in 3.10\n",
      "  __STORAGE_SUFFIX = '_without_storage'\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:33: DeprecationWarning: private variables, such as '_EvaluationCondition__PARTIAL_LOAD_SUFFIX', will be normal attributes in 3.10\n",
      "  __PARTIAL_LOAD_SUFFIX = '_and_partial_load'\n",
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:34: DeprecationWarning: private variables, such as '_EvaluationCondition__PV_SUFFIX', will be normal attributes in 3.10\n",
      "  __PV_SUFFIX = '_and_pv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# import couple of libs some will be useful\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "from datasets import Dataset\n",
    "\n",
    "# import stable_baselines3\n",
    "from stable_baselines3 import PPO, A2C, DDPG, TD3\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb18a81-6582-4643-9219-e4484be57cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"citylearn_challenge_2022_phase_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a88e85e-a447-4d2b-93e4-12410f5be3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1846: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
      "  return np.find_common_type(types, [])  # type: ignore[arg-type]\n"
     ]
    }
   ],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = '/content/neurips-2022-citylearn-challenge/data/citylearn_challenge_2022_phase_2/schema.json'\n",
    "\n",
    "\n",
    "def action_space_to_dict(aspace):\n",
    "    \"\"\" Only for box space \"\"\"\n",
    "    return { \"high\": aspace.high,\n",
    "             \"low\": aspace.low,\n",
    "             \"shape\": aspace.shape,\n",
    "             \"dtype\": str(aspace.dtype)\n",
    "    }\n",
    "\n",
    "def env_reset(env):\n",
    "    observations = env.reset()\n",
    "    action_space = env.action_space\n",
    "    observation_space = env.observation_space\n",
    "    #building_info = env.get_building_information()\n",
    "    #building_info = list(building_info.values())\n",
    "    action_space_dicts = [action_space_to_dict(asp) for asp in action_space]\n",
    "    observation_space_dicts = [action_space_to_dict(osp) for osp in observation_space]\n",
    "    obs_dict = {\"action_space\": action_space_dicts,\n",
    "                \"observation_space\": observation_space_dicts,\n",
    "              #  \"building_info\": building_info,\n",
    "                \"observation\": observations }\n",
    "    return obs_dict\n",
    "\n",
    "import gym\n",
    "\n",
    "# here we init the citylearn env\n",
    "env = CityLearnEnv(schema=\"citylearn_challenge_2023_phase_3_3\")\n",
    "\n",
    "#### IMPORTANT \n",
    "# here we choose the observation we want to take from the building env\n",
    "# we divide observation that are specific to buildings (index_particular)\n",
    "# and observation that are the same for all the buildings (index_commun)\n",
    "\n",
    "index_commun = [0, 2, 19, 4, 8, 24]\n",
    "index_particular = [20, 21, 22, 23]\n",
    "\n",
    "normalization_value_commun = [12, 24, 2, 100, 100, 1]\n",
    "normalization_value_particular = [5, 5, 5, 5]\n",
    "\n",
    "len_tot_index = len(index_commun) + len(index_particular) * 5\n",
    "\n",
    "## env wrapper for stable baselines\n",
    "class EnvCityGym(gym.Env):\n",
    "    \"\"\"\n",
    "    Env wrapper coming from the gym library.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        # get the number of buildings\n",
    "        self.num_buildings = len(env.action_space)\n",
    "\n",
    "        # define action and observation space\n",
    "        self.action_space = gym.spaces.Box(low=np.array([-1] * self.num_buildings), high=np.array([1] * self.num_buildings), dtype=np.float32)\n",
    "\n",
    "        # define the observation space\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0] * len_tot_index), high=np.array([1] * len_tot_index), dtype=np.float32)\n",
    "\n",
    "        # TO THINK : normalize the observation space\n",
    "        self.current_obs = None\n",
    "    def reset(self):\n",
    "        obs_dict = env_reset(self.env)\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        observation = self.get_observation(obs)\n",
    "        \n",
    "        self.current_obs = observation\n",
    "        self.interactions = []\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def get_observation(self, obs):\n",
    "        \"\"\"\n",
    "        We retrieve new observation from the building observation to get a proper array of observation\n",
    "        Basicly the observation array will be something like obs[0][index_commun] + obs[i][index_particular] for i in range(5)\n",
    "\n",
    "        The first element of the new observation will be \"commun observation\" among all building like month / hour / carbon intensity / outdoor_dry_bulb_temperature_predicted_6h ...\n",
    "        The next element of the new observation will be the concatenation of certain observation specific to buildings non_shiftable_load / solar_generation / ...  \n",
    "        \"\"\"\n",
    "        \n",
    "        # we get the observation commun for each building (index_commun)\n",
    "        observation_commun = [obs[0][i]/n for i, n in zip(index_commun, normalization_value_commun)]\n",
    "        observation_particular = [[o[i]/n for i, n in zip(index_particular, normalization_value_particular)] for o in obs]\n",
    "\n",
    "        observation_particular = list(itertools.chain(*observation_particular))\n",
    "        # we concatenate the observation\n",
    "        observation = observation_commun + observation_particular\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        we apply the same action for all the buildings\n",
    "        \"\"\"\n",
    "        # reprocessing action\n",
    "        action = [[act] for act in action]\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        observation = self.get_observation(obs)\n",
    "        \n",
    "        \n",
    "        self.interactions.append({\n",
    "            \"observations\": self.current_obs,\n",
    "            \"next_observations\": self.get_observation(obs),  # Assuming next observation is same as current for simplicity\n",
    "            \"actions\": action,\n",
    "            \"rewards\": reward,\n",
    "            \"dones\": done,\n",
    "            \"info\": info\n",
    "        })\n",
    "        \n",
    "        self.current_obs = observation\n",
    "        \n",
    "        \n",
    "\n",
    "        return observation, sum(reward), done, info\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def test_ppo():\n",
    "\n",
    "    # Modify the petting zoo environment to make a custom observation space (return an array of value for each agent)\n",
    "    \n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=schema)\n",
    "    env = EnvCityGym(env)\n",
    "    \n",
    "    # we load the model\n",
    "    model = PPO.load(\"ppo_citylearn\")\n",
    "\n",
    "    # we reset the environment\n",
    "    obs = env.reset()\n",
    "\n",
    "    nb_iter = 8000\n",
    "\n",
    "    # loop on the number of iteration\n",
    "    for i in range(nb_iter):\n",
    "        # we get the action for each agent\n",
    "        actions = []\n",
    "        for agent in env.possible_agents:\n",
    "            action, _states = model.predict(obs[agent], deterministic=True)\n",
    "\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        actions = {agent: action for agent, action in zip(env.possible_agents, actions)}\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "\n",
    "        # sometimes check the actions and rewards\n",
    "        if i % 100 == 0:\n",
    "            print(\"actions : \", actions)\n",
    "            print(\"rewards : \", rewards)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    final_result = sum(env.citylearnenv.evaluate())/2\n",
    "\n",
    "    print(\"final result : \", final_result)\n",
    "    # launch as main\n",
    "\n",
    "    return final_result\n",
    "    \n",
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def train_ppo():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=schema)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = PPO.load(\"ppo_citylearn\")\n",
    "    except:\n",
    "        model = PPO('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=10000000)\n",
    "\n",
    "    model.save(\"ppo_citylearn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7a85a3-1e5a-4280-8684-402c70dcdbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'citylearn_challenge_2022_phase_2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d339e6f-2109-4033-a653-6b75bf6d201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-stud15/anaconda3/envs/stable_env/lib/python3.9/site-packages/gym/spaces/box.py:112: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = CityLearnEnv(schema=schema)\n",
    "env = EnvCityGym(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5cec19-2113-4816-9c96-29bfa1bf9049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (5,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b2c595-1bd0-4cfd-97a5-b4128e4c9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the policy with PPO algorithm\n",
    "def test_ppo():\n",
    "\n",
    "    # Modify the petting zoo environment to make a custom observation space (return an array of value for each agent)\n",
    "    \n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=Constants.schema_path)\n",
    "    env = EnvCityGym(env)\n",
    "    \n",
    "    # we load the model\n",
    "    model = PPO.load(\"ppo_citylearn\")\n",
    "\n",
    "    # we reset the environment\n",
    "    obs = env.reset()\n",
    "\n",
    "    nb_iter = 8000\n",
    "\n",
    "    # loop on the number of iteration\n",
    "    for i in range(nb_iter):\n",
    "        # we get the action for each agent\n",
    "        actions = []\n",
    "        for agent in env.possible_agents:\n",
    "            action, _states = model.predict(obs[agent], deterministic=True)\n",
    "\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "        actions = {agent: action for agent, action in zip(env.possible_agents, actions)}\n",
    "\n",
    "        # we do a step in the environment\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "\n",
    "        # sometimes check the actions and rewards\n",
    "        if i % 100 == 0:\n",
    "            print(\"actions : \", actions)\n",
    "            print(\"rewards : \", rewards)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    final_result = sum(env.citylearnenv.evaluate())/2\n",
    "\n",
    "    print(\"final result : \", final_result)\n",
    "    # launch as main\n",
    "\n",
    "    return final_result\n",
    "    \n",
    "\n",
    "# function to train the policy with PPO algorithm\n",
    "def train_ppo():\n",
    "\n",
    "    # first we initialize the environment (petting zoo)\n",
    "    env = CityLearnEnv(schema=schema)\n",
    "    env = EnvCityGym(env)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the algorithm\n",
    "\n",
    "    # load model if exist\n",
    "    try:\n",
    "        model = PPO.load(\"ppo_citylearn\")\n",
    "    except:\n",
    "        model = PPO('MlpPolicy', env, verbose=2, gamma=0.99)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=100000)\n",
    "\n",
    "    model.save(\"ppo_citylearn\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6d13f-4777-4c3b-b8ed-105a93342ab3",
   "metadata": {},
   "source": [
    "### Saving Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fd9d39-7d7c-4e38-8938-85cb1088530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = Dataset.from_dict({\n",
    "    'observations': [],\n",
    "    'next_observations': [],\n",
    "    'actions': [],\n",
    "    'rewards': [],\n",
    "    'dones': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701050c2-ea7e-47c8-9a51-9fe5b531fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ee0b74-ed49-47d4-b762-31082a44f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema=schema)\n",
    "env = EnvCityGym(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b62c47-fcca-4abe-a49d-025c0eb4ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf3495d-fd46-49eb-882e-e9d8162bf578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=26, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=26, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c0776e-8f83-4de6-bce7-0e952a60174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5833333333333334,\n",
       " 1.0,\n",
       " 0.08536220341920853,\n",
       " 0.18299999237060546,\n",
       " 0.81,\n",
       " 0.2199999988079071,\n",
       " 0.2989033222198486,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2989033222198486,\n",
       " 0.1541433334350586,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1541433334350586,\n",
       " 1.950581918208627e-08,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.950581918208627e-08,\n",
       " 0.12609000205993653,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.12609000205993653,\n",
       " 0.10914000272750854,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.10914000272750854]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad0ba39-0ba8-4529-b046-e741f7f12dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 190  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f4a3406f280>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8431c404-887b-4326-8c49-35ad6bfb2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = env.interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fde6077-e953-4e17-aaea-35c5848c8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dataset.from_dict({k: [s[k] for s in dataset] for k in dataset[0].keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317bf6af-5130-4035-9891-47e32e638793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[\"dones\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44a278-5292-4465-a52d-9031d38006fb",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3a83-1db7-4709-961c-7a833b86ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = train_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2803203b-1c61-403c-b743-e2ac31e96068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions :  [ 1.         -0.45806432  1.          0.6259297   0.8149345 ]\n",
      "rewards :  -22.450594544410706\n",
      "True\n",
      "8758\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8760 is out of bounds for axis 0 with size 8760",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_iter):\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dones \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(dones)\n",
      "Cell \u001b[0;32mIn[58], line 103\u001b[0m, in \u001b[0;36mEnvCityGym.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    100\u001b[0m action \u001b[38;5;241m=\u001b[39m [[act] \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m action]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# we do a step in the environment\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation(obs)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteractions\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs,\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_observations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_observation(obs),  \u001b[38;5;66;03m# Assuming next observation is same as current for simplicity\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m: info\n\u001b[1;32m    115\u001b[0m })\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/citylearn.py:806\u001b[0m, in \u001b[0;36mCityLearnEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    803\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_actions(actions)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m building, building_actions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildings, actions):\n\u001b[0;32m--> 806\u001b[0m     \u001b[43mbuilding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilding_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_variables()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# NOTE:\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# This call to retrieve each building's observation dictionary is an expensive call especially since the observations \u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# are retrieved again to send to agent but the observations in dict form is needed for the reward function to easily\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# extract building-level values. Can't think of a better way to handle this without giving the reward direct access to\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# env, which is not the best design for competition integrity sake. Will revisit the building.observations() function\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# to see how it can be optimized.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/building.py:970\u001b[0m, in \u001b[0;36mBuilding.apply_actions\u001b[0;34m(self, cooling_device_action, heating_device_action, cooling_storage_action, heating_storage_action, dhw_storage_action, electrical_storage_action)\u001b[0m\n\u001b[1;32m    967\u001b[0m func, args \u001b[38;5;241m=\u001b[39m actions[k]\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/building.py:1134\u001b[0m, in \u001b[0;36mBuilding.update_electrical_storage\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_electrical_storage\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Charge/discharge `electrical_storage` for current time step.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03m        Fraction of `electrical_storage` `capacity` to charge/discharge by.\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m     energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(action\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage\u001b[38;5;241m.\u001b[39mcapacity, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownward_electrical_flexibility\u001b[49m)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage\u001b[38;5;241m.\u001b[39mcharge(energy)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable_env/lib/python3.9/site-packages/citylearn/building.py:557\u001b[0m, in \u001b[0;36mBuilding.downward_electrical_flexibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownward_electrical_flexibility\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Available distributed energy resource capacity to satisfy electric loads while considering power outage at current time step.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    It is the sum of solar generation and any discharge from electrical storage, less electricity consumption by cooling, heating, \u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m    dhw and non-shfitable load devices as well as charging electrical storage. When there is no power outage, the returned value \u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m    is `np.inf`.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     capacity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolar_generation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m (\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcooling_device\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    559\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheating_device\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step] \n\u001b[1;32m    560\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdhw_device\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_shiftable_load_device\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m     capacity \u001b[38;5;241m=\u001b[39m capacity \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_outage \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m    566\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownward_electrical_flexibility must be >= 0.0!\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime step:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, outage:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_outage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, capacity:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapacity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m solar:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolar_generation[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m                             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-shiftable:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_shiftable_load_device\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m    573\u001b[0m                                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m battery:, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melectrical_storage\u001b[38;5;241m.\u001b[39melectricity_consumption[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8760 is out of bounds for axis 0 with size 8760"
     ]
    }
   ],
   "source": [
    "\n",
    "# simple run though the env with our PPO policy and we sometimes print our actions / reward to get a sense of what we are doing\n",
    "env = CityLearnEnv(schema=schema)\n",
    "env = EnvCityGym(env)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "model = PPO.load(\"ppo_citylearn\")\n",
    "\n",
    "nb_iter = 100000000\n",
    "\n",
    "reward_tot = 0\n",
    "\n",
    "for i in range(nb_iter):\n",
    "\n",
    "    action = model.predict(obs)[0]\n",
    "        \n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    \n",
    "    if dones is not False:\n",
    "        print(dones)\n",
    "        print(i)\n",
    "    reward_tot += rewards \n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(\"actions : \", action)\n",
    "        print(\"rewards : \", rewards)\n",
    "\n",
    "print(sum(env.env.evaluate())/2)\n",
    "print(reward_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4792e6e-4638-4264-a9d4-71ac336a36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660add4d-4119-4de3-82d4-149f91a14c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e348c5d-ae56-4ea7-9260-4fce67da40da",
   "metadata": {},
   "source": [
    "df_evaluate =env.env.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd474d7-3a9c-4208-bb2e-dab67e46a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluate[df_evaluate.cost_function==\"cost_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcdb40-2b34-4130-9a09-8a0686884af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d83fc-5cb8-441e-b6ae-32f10e0c5f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
